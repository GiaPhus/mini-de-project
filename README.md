# Mini Data Engineering Project — Orders Revenue Pipeline

## 1. Goal
Build a mini batch pipeline to compute **daily revenue** from raw CSV files:
- Standardize & clean input
- Validate data quality (reject invalid records)
- Compute daily revenue for BI
- Produce a **quality_report.json** for monitoring


---

## 2. Input data
You will find sample inputs in `/data/`:
- `orders_2024-01-01.csv`
- `order_items_2024-01-01.csv`

Data issues may exist (duplicates, nulls, negative prices, orphan items).

---

## 3. Requirements

### 3.1 Staging (clean + standardize)
**Orders**
- Parse datetime fields (e.g. `order_date`, `ingested_at`)
- Cast types
- Standardize `status`
- **Deduplicate by `order_id`**: keep the row with latest `ingested_at`

**Order items**
- Parse datetime fields (e.g. `ingested_at`)
- Cast types

### 3.2 Data Quality rules (reject invalid rows)
- `orders`: `order_id`, `customer_id`, `order_date`, `status` must be NOT NULL
- `order_items`: `quantity` must be NOT NULL, `unit_price` must be > 0
- Orphan items: `order_items.order_id` not found in orders => reject

### 3.3 Business logic
- Revenue counts only for orders with `status = 'completed'`
- `amount = quantity * unit_price`
- Output a daily mart:
  - `order_date`
  - `total_revenue`
  - `orders_count`

### 3.4 Outputs (write to `/output/`)
- `daily_revenue.csv`
- `rejected_orders.csv` (if any)
- `rejected_items.csv` (if any)
- `quality_report.json`

### 3.5 Idempotency (basic)
Your pipeline should be safe to rerun:
- Running it multiple times should not duplicate outputs
- Recommended: overwrite output files / overwrite partition for the run date

---

## 4. Suggested repo structure
(You can adjust, but keep it clean.)

mini-de-project/
├── README.md
├── requirements.txt
├── data/
│   ├── orders_2024-01-01.csv
│   └── order_items_2024-01-01.csv
├── etl/
│   └── run_pipeline.py
├── sql/
│   ├── models/
│   └── checks/
│       └── dq_checks.sql
└── output/
    └── (generated by pipeline)

---

## 5. How to run

### 5.1 Prerequisites
- Python 3.10+ installed
- Recommended: use a virtual environment

### 5.2 Windows PowerShell (recommended)
From the project root folder:

```powershell
python -m venv .venv
.\.venv\Scripts\Activate.ps1
pip install -r requirements.txt

python etl\run_pipeline.py --run-date 2024-01-01 --input-dir data --output-dir output
